---
layout: post
mathjax: true
title:  "Strategies for forecasting multiple heterogeneous time series"
date:   2017-12-22
categories: data
description: I briefly explore two approaches on one of the most challening problems in the field fo time series forecasting 
tags: [time series, LSTM, autoencoder]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" ></script>

<img src="/assets/images/macbook_timeseries.png" width="640"/>

The forecasting of multiple heterogeneous time series is one of the most challenging problems in the field of forecasting. So much that the Wikipedia foundation launched a [Kaggle completion](https://www.kaggle.com/c/web-traffic-time-series-forecasting#description) on the matter. [As did the German Drugstore, Rossman](https://www.kaggle.com/c/rossmann-store-sales)
  
Until recently, most forecasting problems involved a single time series or up to a couple of dozens. So for example, let's say you want to forecast how many Macbooks Pros will the Apple Store in Palo Alto sell in a given day. You would collect all the past daily sales for that store and fit a single model. If you also want to forecast it for two other Apple Stores, you could build two other separate models, and so on. 

But what if you want to forecast the sales of all Macbooks types for all Apple stores in the world? Then we would be dealing with thousands of time series. Or in the case wikipedia web traffic forecasting, we could be talking millions of time series. In the age of Big Data this kind of situation is becoming the norm. 

When we reach this kinds of numbers it becomes expensive in terms of resources and time to build a single model for each time series. What if we try to build one single model for all of them? It sounds like a promissing approach. Not so fast. 

 When we train all of them together what we are assuming they come from the same data generating process. In other words, that each time-series is an event generated by same probability distribution and we expect them to somewhat homogeneous. 

 In real world applications, this is not necessarly the case. The underlying mechanisms driving sales of MacBook Airs in Italy can be very very different than that of Macbook Pros in Vancourver. You you put both those guys in the same model, you could be adding a lot of noise and it difficulting the learning process of the algorithm. 

So how is the community tackling this kind of problem? From the research I did, I found two interesting approaches. Both use LSTMs. These are neural networks that work great on sequence data. They are used in many applications of Natural Language Processing, like machine translation and speech recognition. If you're interested in digging deeper I recomend this excelent [blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
 
One [approach](https://arxiv.org/pdf/1710.03222.pdf) involves clustering similar time series together and then training one model for each cluster. This is a balance between one model for each series and one huge model for them all. The assumption is that by training models on similar sequences yourreduce noise and set an easier terrain for it to learn structure. 

<img src="/assets/images/cluster_timeseries.png" width="640"/>
 
The [other approach](https://eng.uber.com/neural-networks/) is a bit more complicated but also a bit more elegant. It involves two steps. First, train an LSTM autoencoder to learn an embedding of all the time series. In other words, try to find a lower dimension representation of the data which contains all the information needed. to replicate itself. Then, concatenate it with any other external features (like temperature) feed it into a multi-layer perceptron.  

<img src="/assets/images/lstm-mlp-autoencoder.png" height="100"/>

This does not mean that simply applying what is proposed in those articles is an easy task. Both of them invovle training deep neural networks with many parameters. Some say this aspect is more art than science. 

I plan to explore both of these approaches in future posts on the Wikpedia web traffic dataset. If you have any comments, please reach out to me!







